{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Load and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Converts to PyTorch tensor and scales to [0, 1]\n",
    "    transforms.Normalize((0.5,), (0.5,)) # Normalizes to have mean=0.5, std=0.5\n",
    "])\n",
    "\n",
    "# Load FashionMNIST datasets\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display first 8 images from the training set\n",
    "fig, axes = plt.subplots(1, 8, figsize=(20, 4))\n",
    "for i in range(8):\n",
    "    img, label = train_dataset[i]\n",
    "    axes[i].imshow(img.squeeze(), cmap='gray') # img is a 1x28x28 tensor, squeeze removes dimensions of size 1\n",
    "    axes[i].set_title(f'Example: {i+1}')\n",
    "    axes[i].axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionMNISTModel(\n",
      "  (fc1): Linear(in_features=784, out_features=300, bias=True)\n",
      "  (fc2): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FashionMNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionMNISTModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 300) # Flatten the input in the forward method\n",
    "        self.fc2 = nn.Linear(300, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28) # Flatten the input\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(self.fc3(x), dim=1)\n",
    "\n",
    "model = FashionMNISTModel().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 300]         235,500\n",
      "            Linear-2                  [-1, 100]          30,100\n",
      "            Linear-3                   [-1, 10]           1,010\n",
      "================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 1.02\n",
      "Estimated Total Size (MB): 1.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# Assuming your model is already defined and sent to the device\n",
    "summary(model, input_size=(1, 28*28))  # For FashionMNIST, input size is (channels, H*W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_visualization2.png'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Create a dummy input tensor appropriate for your model\n",
    "dummy_input = torch.randn(1, 28*28, device=device)\n",
    "# Get the output by forwarding the dummy input through the model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "\n",
    "# Visualize the graph; requires the output and parameters\n",
    "graph = make_dot(output, params=dict(list(model.named_parameters()) + [('input', dummy_input)]))\n",
    "graph.render('model_visualization2', format='png')  # Saves the graph to a file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "# Export the model\n",
    "torch.onnx.export(model, dummy_input, 'model.onnx', export_params=True, opset_version=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the basic block of ResNet\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        # First convolutional layer of the block\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Second convolutional layer of the block\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
    "            # This adds a convolutional layer to the shortcut path to match the dimensions\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)  # Element-wise addition\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "# Define a simple ResNet model for FashionMNIST\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        # Initial convolutional layer before the blocks\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Layers of blocks\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\n",
    "        # Classifier\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "# Number of blocks in each layer\n",
    "num_blocks = [2, 2, 2, 2]\n",
    "\n",
    "# Create the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model2 = ResNet(BasicBlock, num_blocks).to(device)\n",
    "print(model2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 28, 28]             576\n",
      "       BatchNorm2d-2           [-1, 64, 28, 28]             128\n",
      "            Conv2d-3           [-1, 64, 28, 28]          36,864\n",
      "       BatchNorm2d-4           [-1, 64, 28, 28]             128\n",
      "            Conv2d-5           [-1, 64, 28, 28]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 28, 28]             128\n",
      "        BasicBlock-7           [-1, 64, 28, 28]               0\n",
      "            Conv2d-8           [-1, 64, 28, 28]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 28, 28]             128\n",
      "           Conv2d-10           [-1, 64, 28, 28]          36,864\n",
      "      BatchNorm2d-11           [-1, 64, 28, 28]             128\n",
      "       BasicBlock-12           [-1, 64, 28, 28]               0\n",
      "           Conv2d-13          [-1, 128, 14, 14]          73,728\n",
      "      BatchNorm2d-14          [-1, 128, 14, 14]             256\n",
      "           Conv2d-15          [-1, 128, 14, 14]         147,456\n",
      "      BatchNorm2d-16          [-1, 128, 14, 14]             256\n",
      "           Conv2d-17          [-1, 128, 14, 14]           8,192\n",
      "      BatchNorm2d-18          [-1, 128, 14, 14]             256\n",
      "       BasicBlock-19          [-1, 128, 14, 14]               0\n",
      "           Conv2d-20          [-1, 128, 14, 14]         147,456\n",
      "      BatchNorm2d-21          [-1, 128, 14, 14]             256\n",
      "           Conv2d-22          [-1, 128, 14, 14]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 14, 14]             256\n",
      "       BasicBlock-24          [-1, 128, 14, 14]               0\n",
      "           Conv2d-25            [-1, 256, 7, 7]         294,912\n",
      "      BatchNorm2d-26            [-1, 256, 7, 7]             512\n",
      "           Conv2d-27            [-1, 256, 7, 7]         589,824\n",
      "      BatchNorm2d-28            [-1, 256, 7, 7]             512\n",
      "           Conv2d-29            [-1, 256, 7, 7]          32,768\n",
      "      BatchNorm2d-30            [-1, 256, 7, 7]             512\n",
      "       BasicBlock-31            [-1, 256, 7, 7]               0\n",
      "           Conv2d-32            [-1, 256, 7, 7]         589,824\n",
      "      BatchNorm2d-33            [-1, 256, 7, 7]             512\n",
      "           Conv2d-34            [-1, 256, 7, 7]         589,824\n",
      "      BatchNorm2d-35            [-1, 256, 7, 7]             512\n",
      "       BasicBlock-36            [-1, 256, 7, 7]               0\n",
      "           Conv2d-37            [-1, 512, 4, 4]       1,179,648\n",
      "      BatchNorm2d-38            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-39            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-40            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-41            [-1, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-42            [-1, 512, 4, 4]           1,024\n",
      "       BasicBlock-43            [-1, 512, 4, 4]               0\n",
      "           Conv2d-44            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-45            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-46            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-47            [-1, 512, 4, 4]           1,024\n",
      "       BasicBlock-48            [-1, 512, 4, 4]               0\n",
      "           Linear-49                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 11,172,810\n",
      "Trainable params: 11,172,810\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 8.79\n",
      "Params size (MB): 42.62\n",
      "Estimated Total Size (MB): 51.41\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Assuming your model is already defined and sent to the device\n",
    "summary(model2, input_size=(1, 28, 28))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming model2 is your defined ResNet model\n",
    "# Ensure the model is in evaluation mode\n",
    "model2.eval()\n",
    "\n",
    "# Create a dummy input tensor with the correct shape [batch_size, channels, height, width]\n",
    "# For a single grayscale image of size 28x28 pixels\n",
    "dummy_input = torch.randn(1, 1, 28, 28, device=device)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    model2, \n",
    "    dummy_input, \n",
    "    'model2.onnx', \n",
    "    export_params=True, \n",
    "    opset_version=11,\n",
    "    input_names=['input'],  # You can specify the input names\n",
    "    output_names=['output'],  # You can also specify the output names\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}  # This makes the model's batch size dynamic\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1982647329568863\n",
      "Epoch 2, Loss: 0.3017524480819702\n",
      "Epoch 3, Loss: 0.4683306813240051\n",
      "Epoch 4, Loss: 0.6021705865859985\n",
      "Epoch 5, Loss: 0.2026500701904297\n",
      "Epoch 6, Loss: 0.27873238921165466\n",
      "Epoch 7, Loss: 0.41515621542930603\n",
      "Epoch 8, Loss: 0.45533138513565063\n",
      "Epoch 9, Loss: 0.23996442556381226\n",
      "Epoch 10, Loss: 0.17839068174362183\n",
      "Epoch 11, Loss: 0.31937071681022644\n",
      "Epoch 12, Loss: 0.3001250624656677\n",
      "Epoch 13, Loss: 0.4046729505062103\n",
      "Epoch 14, Loss: 0.20801527798175812\n",
      "Epoch 15, Loss: 0.086492158472538\n",
      "Training completed in: 204.431 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Assuming the PyTorch training loop here\n",
    "\n",
    "for epoch in range(15):  # Assuming you're running for 15 epochs\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)  # Move data to the device\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# Optionally, add validation logic inside your training loop\n",
    "\n",
    "# End timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the duration\n",
    "duration = end_time - start_time\n",
    "print(f\"Training completed in: {duration:.3f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8853\n",
      "Precision: 0.8865\n",
      "Recall: 0.8853\n",
      "F1 Score: 0.8852\n",
      "Confusion Matrix:\n",
      " [[877   0  16  11   3   2  86   0   5   0]\n",
      " [  8 972   1  12   3   0   3   0   1   0]\n",
      " [ 25   0 844   7  78   0  45   0   1   0]\n",
      " [ 41   5  15 877  27   0  32   0   3   0]\n",
      " [  1   0 101  32 820   0  45   0   1   0]\n",
      " [  1   0   0   0   0 965   0  17   0  17]\n",
      " [137   1 107  17  76   0 661   0   1   0]\n",
      " [  0   0   0   0   0  37   0 929   0  34]\n",
      " [ 12   1   6   7   4   7  17   5 941   0]\n",
      " [  1   0   0   0   0   8   0  24   0 967]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts to PyTorch tensor and scales to [0, 1]\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalizes to have mean=0.5, std=0.5\n",
    "])\n",
    "\n",
    "# Load FashionMNIST datasets\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Assuming model2 is your defined ResNet model\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move model to appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Initialize lists to monitor test results\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# No need to track gradients for validation, which saves memory and computations\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Get prediction from the maximum value\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Append batch prediction results\n",
    "        y_pred.extend(predicted.view(-1).cpu().numpy())\n",
    "        y_true.extend(labels.view(-1).cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "y_pred = np.array(y_pred)\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print('Confusion Matrix:\\n', conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2131364643573761\n",
      "Epoch 2, Loss: 0.25097158551216125\n",
      "Epoch 3, Loss: 0.1264793425798416\n",
      "Epoch 4, Loss: 0.08945557475090027\n",
      "Epoch 5, Loss: 0.1915014684200287\n",
      "Epoch 6, Loss: 0.07213769853115082\n",
      "Epoch 7, Loss: 0.06508186459541321\n",
      "Epoch 8, Loss: 0.007715204730629921\n",
      "Epoch 9, Loss: 0.06406931579113007\n",
      "Epoch 10, Loss: 0.03312810882925987\n",
      "Epoch 11, Loss: 0.04051384702324867\n",
      "Epoch 12, Loss: 0.040204647928476334\n",
      "Epoch 13, Loss: 0.0004991881432943046\n",
      "Epoch 14, Loss: 0.01711859367787838\n",
      "Epoch 15, Loss: 0.006040805950760841\n",
      "Training completed in: 476.395 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "optimizer = optim.Adam(model2.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Assuming the PyTorch training loop here\n",
    "\n",
    "for epoch in range(15):  # Assuming you're running for 15 epochs\n",
    "    model2.train()\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)  # Move data to the device\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model2(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# Optionally, add validation logic inside your training loop\n",
    "\n",
    "# End timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the duration\n",
    "duration = end_time - start_time\n",
    "print(f\"Training completed in: {duration:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9341\n",
      "Precision: 0.9351\n",
      "Recall: 0.9341\n",
      "F1 Score: 0.9344\n",
      "Confusion Matrix:\n",
      " [[858   0  19   7   8   1 105   0   2   0]\n",
      " [  0 985   0   6   2   0   6   0   1   0]\n",
      " [ 11   2 901   7  37   0  42   0   0   0]\n",
      " [ 12   1  10 931  26   1  19   0   0   0]\n",
      " [  0   0  16  15 926   0  43   0   0   0]\n",
      " [  0   0   1   0   0 983   0   9   1   6]\n",
      " [ 60   1  32  18  55   0 828   0   6   0]\n",
      " [  0   0   0   0   0   3   0 964   0  33]\n",
      " [  1   1   2   2   4   2   6   0 982   0]\n",
      " [  0   0   0   0   0   2   0  14   1 983]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts to PyTorch tensor and scales to [0, 1]\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalizes to have mean=0.5, std=0.5\n",
    "])\n",
    "\n",
    "# Load FashionMNIST datasets\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Assuming model2 is your defined ResNet model\n",
    "# Ensure the model is in evaluation mode\n",
    "model2.eval()\n",
    "\n",
    "# Move model to appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model2.to(device)\n",
    "\n",
    "# Initialize lists to monitor test results\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# No need to track gradients for validation, which saves memory and computations\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model2(images)\n",
    "\n",
    "        # Get prediction from the maximum value\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Append batch prediction results\n",
    "        y_pred.extend(predicted.view(-1).cpu().numpy())\n",
    "        y_true.extend(labels.view(-1).cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "y_pred = np.array(y_pred)\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print('Confusion Matrix:\\n', conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Explicitly setting the device to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Assuming the model definition is already provided\n",
    "model = FashionMNISTModel().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(15):  # Assuming you're running for 15 epochs\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        # Since we're running on CPU, there's no need to call .to(device) on the data and targets\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# End timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the duration\n",
    "duration = end_time - start_time\n",
    "print(f\"Training completed in: {duration:.3f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
